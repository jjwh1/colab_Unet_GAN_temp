import torch
import torch.nn as nn
import torch.nn.functional as F


class UNetGenerator(nn.Module):
    def __init__(self, input_channels=4, output_channels=3):
        super(UNetGenerator, self).__init__()

        # Contracting path
        self.enc1 = self.conv_block(input_channels, 64)
        self.enc2 = self.conv_block(64, 128)



        # Bottleneck
        self.bottleneck = self.conv_block(128, 256)

        # Expansive path


        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=4)
        self.dec2 = self.conv_block(256, 128)

        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4)
        self.dec1 = self.conv_block(128, 64)

        # Final layer
        self.final_layer = nn.Conv2d(64, output_channels, kernel_size=1)  # kernel_size = 3, padding = 1 해보기

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # Encoder path
        enc1 = self.enc1(x)
        enc2 = self.enc2(F.max_pool2d(enc1, 4))   # max_pool2d : stride 기본값 = kernel_size

        # Bottleneck
        bottleneck = self.bottleneck(F.max_pool2d(enc2, 4))

        # Decoder path

        dec2 = self.upconv2(bottleneck)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.dec2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.dec1(dec1)

        # Final layer
        return torch.sigmoid(self.final_layer(dec1)), enc1, enc2, bottleneck, dec1, dec2          


class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),
            # 이거 224x224 기준이면 (batch_size, 1, 11, 11) 로 최종 shape 나옴. 일반적인(이상적인) Discriminator는 스칼라 하나만 뱉는거니 (batch_size,1,1,1)이 맞긴한듯? 이것도 틀린건 아닌듯함. 다른 부분도 보겠다는 거니.....
            # ㄴ그럼 이게 patch gan과 같은 거??? ㅇㅇ
            nn.Sigmoid()
            # Discriminator의 출력이 로짓(logit) 값(예: [-∞, ∞] 범위)이기 때문에, 이를 확률 값으로 변환하기 위해 sigmoid()를 사용
        )

    def forward(self, x):
        return self.model(x).view(-1, 1).squeeze(1)  # view까지 한 결과 (Batch * 11 * 11, 1) -> 최종 shape : (Batch * 11 * 11,)

    nn.MaxPool2d(2)